{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max 1000 property links per suburb\n",
    "\n",
    "def get_property_links(suburb, postcode, max_pages=50):\n",
    "    base_url = f'https://www.domain.com.au/rent/{suburb}-vic-{postcode}/?sort=price-desc&page='\n",
    "    property_links = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = base_url + str(page)\n",
    "\n",
    "        response = requests.get(url, headers={'User-Agent': 'PostmanRuntime/7.6.0'})\n",
    "\n",
    "        if response.status_code == 403:\n",
    "            print(f\"RATE_LIMITING: Failed to retrieve page {page}, {suburb}, {postcode}. Status code: {response.status_code}. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        if response.status_code == 404:\n",
    "            print(f\"SUBURB_NOT_FOUND: Failed to retrieve page {page}, {suburb}, {postcode} likely due to incorrect suburb name. Status code: {response.status_code}. Exiting loop.\")\n",
    "            break\n",
    "        \n",
    "        # Check if the response status code is not 200\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page}, {suburb}, {postcode}. Status code: {response.status_code}. Exiting loop.\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        listings = soup.find_all('a', class_='address')\n",
    "\n",
    "        if not listings:\n",
    "            print(f\"NO LISTINGS: No listings found on page {page}, {suburb}, {postcode}. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        for listing in listings:\n",
    "            link = listing.get('href')\n",
    "            if link:\n",
    "                property_links.append(link)\n",
    "\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return property_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_property_links(postcode_mapping):\n",
    "    all_property_links = []\n",
    "    \n",
    "    for postcode, suburbs in postcode_mapping.items():\n",
    "        for suburb in suburbs:\n",
    "            property_links = get_property_links(suburb, postcode)\n",
    "            # You can choose to store suburb and postcode with each link if needed\n",
    "            all_property_links.append(property_links)\n",
    "    \n",
    "    return all_property_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_property_details(soup):\n",
    "    # Locate the JSON data embedded in the HTML\n",
    "    script_data = soup.find('script', id='__NEXT_DATA__').string\n",
    "    json_data = json.loads(script_data)\n",
    "    \n",
    "    # Extract necessary details\n",
    "    layout_props = json_data['props']['pageProps']['layoutProps']\n",
    "    property_details = layout_props[\"digitalData\"][\"page\"][\"pageInfo\"][\"property\"]\n",
    "    component_props = json_data['props']['pageProps']['componentProps']\n",
    "    \n",
    "    data = {\n",
    "        'title': layout_props.get('title'),\n",
    "        'description': layout_props.get('description'),\n",
    "        'street_adress': property_details.get('address'),\n",
    "        'suburb': property_details.get('suburb'),\n",
    "        'postcode': property_details.get('postcode'),\n",
    "        'price': property_details.get('price'),\n",
    "        'bedrooms': property_details.get('bedrooms'),\n",
    "        'bathrooms': property_details.get('bathrooms'),\n",
    "        'parking': property_details.get('parking'),\n",
    "        'primary_property_type': property_details.get('primaryPropertyType'),\n",
    "        'property_features': property_details.get('propertyFeatures'),\n",
    "        'structured_features': property_details.get('structuredFeatures', []),\n",
    "        'video_count': property_details.get('videoCount'),\n",
    "        'photo_count': property_details.get('photoCount'),\n",
    "        'date_listed': property_details.get('dateListed'),\n",
    "        'days_listed': property_details.get('daysListed'),\n",
    "        'floor_plans_count': property_details.get('floorPlansCount'),\n",
    "        'virtual_tour': property_details.get('virtualTour'),\n",
    "        'nbn_details': layout_props.get('nbnDetails'),\n",
    "        'nearby_schools': component_props.get('schoolCatchment', {}).get('schools', [])\n",
    "    }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_properties(property_links):\n",
    "    # Create an empty list to store the property details\n",
    "    all_properties = []\n",
    "\n",
    "    for url in property_links:\n",
    "        # Fetch the page content\n",
    "        response = requests.get(url, headers={'User-Agent': 'PostmanRuntime/7.6.0'})\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            print(f\"RATE_LIMITING: Failed to retrieve page {url}. Status code: {response.status_code}. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        # Check if the response status code is not 200\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {url}. Status code: {response.status_code}. Exiting loop.\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract the property details\n",
    "        property_data = extract_property_details(soup)\n",
    "        all_properties.append(property_data)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(all_properties)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the postcode mapping from the JSON file\n",
    "with open('../../data/landing/postcode_mapping.json', 'r') as json_file:\n",
    "    postcode_mapping = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_property_links_threaded(postcode_mapping, max_workers=5):\n",
    "    all_property_links = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for postcode, suburbs in postcode_mapping.items():\n",
    "            for suburb in suburbs:\n",
    "                futures.append(executor.submit(get_property_links, suburb, postcode))\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            all_property_links.extend(future.result())\n",
    "    \n",
    "    return all_property_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_properties_threaded(property_links, max_workers=5):\n",
    "    all_properties = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for url in property_links:\n",
    "            futures.append(executor.submit(scrape_properties, [url]))\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            all_properties.append(future.result())\n",
    "    \n",
    "    return pd.concat(all_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: This will take a long time to run (hours)\n",
    "# Status codes 404 are common because of duplicate suburb names, so don't worry about them\n",
    "# Status codes 403 are rate limiting, so you may need to slow down your requests\n",
    "\n",
    "property_links = collect_all_property_links_threaded(postcode_mapping, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame and save to CSV\n",
    "links_df = pd.DataFrame(property_links, columns=['property_link'])\n",
    "links_df.drop_duplicates(inplace=True)\n",
    "links_df.to_parquet('../../data/raw/property_links.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = pd.read_parquet('../../data/raw/property_links.parquet')\n",
    "links_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m property_df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_properties_threaded\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinks_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproperty_link\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mscrape_properties_threaded\u001b[0;34m(property_links, max_workers)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m property_links:\n\u001b[1;32m      7\u001b[0m     futures\u001b[38;5;241m.\u001b[39mappend(executor\u001b[38;5;241m.\u001b[39msubmit(scrape_properties, [url]))\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(futures):\n\u001b[1;32m     11\u001b[0m     all_properties\u001b[38;5;241m.\u001b[39mappend(future\u001b[38;5;241m.\u001b[39mresult())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "property_df = scrape_properties_threaded(links_df['property_link'], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_df.to_parquet(f'../../data/raw/property_details.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
